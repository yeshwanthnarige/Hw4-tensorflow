# Hw4-tensorflow



**Student:Yeshwanth narige  
id 700764035
  

---

## ğŸ“– Overview  
This repository contains solutions for **Home AssignmentÂ 4**  
1. **Q1: NLP Preprocessing Pipeline**  
2. **Q2: Named Entity Recognition with spaCy**  
3. **Q3: ScaledÂ Dotâ€‘Product Attention**  
4. **Q4: Sentiment Analysis using HuggingFace Transformers**  

Each task includes:  
- A selfâ€‘contained Python script or notebook.  
- A clear README section with instructions.  
- Example input, expected output, and short answers to conceptual questions.  

---

## ğŸ—‚ Project Structure  
```
.
â”œâ”€â”€ Q1_NLP_Preprocessing/
â”‚   â”œâ”€â”€ preprocess.py       # tokenization, stopword removal, stemming
â”‚   â””â”€â”€ README.md           # task description & how to run
â”‚
â”œâ”€â”€ Q2_NER_spaCy/
â”‚   â”œâ”€â”€ ner_spacy.py        # spaCy pipeline for NER extraction
â”‚   â””â”€â”€ README.md           # task description & how to run
â”‚
â”œâ”€â”€ Q3_Scaled_DotProduct_Attention/
â”‚   â”œâ”€â”€ attention.py        # implement & test scaled dot-product attention
â”‚   â””â”€â”€ README.md           # task description & how to run
â”‚
â””â”€â”€ Q4_Sentiment_Transformers/
    â”œâ”€â”€ sentiment.py        # HuggingFace pipeline example
    â””â”€â”€ README.md           # task description & how to run
```

---

## âš™ï¸ Requirements & Setup  

1. **Clone the repo**  
   ```bash
   git clone https://github.com/yourâ€‘username/yourâ€‘repo.git
   cd yourâ€‘repo
   ```
2. **Install dependencies**  
   ```bash
   pip install nltk spacy transformers numpy sklearn
   python -m spacy download en_core_web_sm
   ```
3. **Run each module**  
   - Navigate into one of `Q1_â€¦`, `Q2_â€¦`, etc.  
   - Execute the script:  
     ```bash
     python preprocess.py     # for Q1
     python ner_spacy.py      # for Q2
     python attention.py      # for Q3
     python sentiment.py      # for Q4
     ```

---

## ğŸ“‹ Task Summaries  

### Q1: NLP Preprocessing Pipeline  
- **Steps:**  
  1. Tokenize a sample sentence.  
  2. Remove English stopwords.  
  3. Apply Porter stemming.  
- **Short Answers:**  
  1. Stemming vs. Lemmatization (e.g. â€œrunningâ€â†’â€œrunâ€ via stemmer vs. lemmas).  
  2. When stopwords help (topic modeling) vs. hurt (sentiment flip â€œnotâ€).  

### Q2: Named Entity Recognition  
- **Steps:**  
  1. Load `en_core_web_sm`.  
  2. Extract entities, labels, and character spans.  
- **Short Answers:**  
  1. NER (semantic spans) vs. POS tagging (syntactic labels).  
  2. Applications: financial news extraction, searchâ€‘engine entity linking.  

### Q3: Scaled Dotâ€‘Product Attention  
- **Implementation:**  
  1. Compute QÂ·Káµ€  
  2. Scale by âˆšd  
  3. Softmax â†’ attention weights  
  4. Multiply weights by V  
- **Short Answers:**  
  1. âˆšd prevents softmax saturation in highâ€‘dim.  
  2. Selfâ€‘attention models longâ€‘range word dependencies.  

### Q4: Sentiment Analysis with Transformers  
- **Steps:**  
  1. Load HuggingFace `pipeline("sentiment-analysis")`.  
  2. Classify example sentence.  
- **Short Answers:**  
  1. BERT = encoderâ€‘only, GPT = decoderâ€‘only.  
  2. Preâ€‘trained models save compute, data, and provide strong baselines.  

---
