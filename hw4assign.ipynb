{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEnQwCSfgeeZ",
        "outputId": "102e74c2-ff69-4825-f901-f1920ff848e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence:\n",
            "  NLP techniques are used in virtual assistants like Alexa and Siri. \n",
            "\n",
            "1) Original Tokens:\n",
            "  ['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri', '.'] \n",
            "\n",
            "2) Tokens Without Stopwords:\n",
            "  ['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri', '.'] \n",
            "\n",
            "3) Stemmed Words:\n",
            "  ['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri', '.'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === Install & download required packages ===\n",
        "#!pip install --quiet nltk spacy transformers\n",
        "#!python -m spacy download --quiet en_core_web_sm\n",
        "\n",
        "# === Imports ===\n",
        "import numpy as np\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "# Download the missing 'punkt_tab' data package\n",
        "nltk.download('punkt_tab', quiet=True) # This line is added to download the missing data\n",
        "\n",
        "# === Q1: NLP Preprocessing Pipeline ===\n",
        "def nlp_preprocess(sentence):\n",
        "    print(\"Original sentence:\")\n",
        "    print(\" \", sentence, \"\\n\")\n",
        "\n",
        "    # 1. Tokenize\n",
        "    tokens = word_tokenize(sentence)\n",
        "    print(\"1) Original Tokens:\")\n",
        "    print(\" \", tokens, \"\\n\")\n",
        "\n",
        "    # 2. Remove stopwords\n",
        "    stops = set(stopwords.words('english'))\n",
        "    tokens_nostop = [t for t in tokens if t.lower() not in stops]\n",
        "    print(\"2) Tokens Without Stopwords:\")\n",
        "    print(\" \", tokens_nostop, \"\\n\")\n",
        "\n",
        "    # 3. Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(t) for t in tokens_nostop]\n",
        "    print(\"3) Stemmed Words:\")\n",
        "    print(\" \", stems, \"\\n\")\n",
        "\n",
        "# Run Q1 on the given sentence\n",
        "sentence_q1 = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "nlp_preprocess(sentence_q1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Stemming vs. Lemmatization\n",
        "\n",
        "Stemming crudely chops off word endings using simple rules (e.g. Porter stemmer):\n",
        "\n",
        "“running” → “run” → “run” or sometimes “runn”\n",
        "\n",
        "Lemmatization uses vocabulary and morphological analysis to return the base (dictionary) form:\n",
        "\n",
        "“running” → “run”\n",
        "\n",
        "2. When to remove stop words\n",
        "\n",
        "Useful: in tasks like topic modeling or keyword extraction where common words add noise and do not carry meaning.\n",
        "\n",
        "Harmful: in tasks like sentiment analysis or question answering, where words like “not,” “very,” or “but” critically affect meaning if removed."
      ],
      "metadata": {
        "id": "zZigBJPTi-Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === Q2: Named Entity Recognition with spaCy ===\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def ner_example(text):\n",
        "    doc = nlp(text)\n",
        "    print(f\"Input sentence:\\n  {text}\\n\")\n",
        "    print(\"Detected Entities:\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\" - Text: '{ent.text}'\\n\"\n",
        "              f\"   Label: {ent.label_}\\n\"\n",
        "              f\"   Char pos: ({ent.start_char}, {ent.end_char})\\n\")\n",
        "\n",
        "# Run Q2 on the given sentence\n",
        "text_q2 = (\"Barack Obama served as the 44th President of the United States \"\n",
        "           \"and won the Nobel Peace Prize in 2009.\")\n",
        "ner_example(text_q2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mul2gbnNiVXC",
        "outputId": "0fa88a72-5a08-4c1d-ff54-9651760df055"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence:\n",
            "  Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\n",
            "\n",
            "Detected Entities:\n",
            " - Text: 'Barack Obama'\n",
            "   Label: PERSON\n",
            "   Char pos: (0, 12)\n",
            "\n",
            " - Text: '44th'\n",
            "   Label: ORDINAL\n",
            "   Char pos: (27, 31)\n",
            "\n",
            " - Text: 'the United States'\n",
            "   Label: GPE\n",
            "   Char pos: (45, 62)\n",
            "\n",
            " - Text: 'the Nobel Peace Prize'\n",
            "   Label: WORK_OF_ART\n",
            "   Char pos: (71, 92)\n",
            "\n",
            " - Text: '2009'\n",
            "   Label: DATE\n",
            "   Char pos: (96, 100)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. NER vs. POS Tagging**  \n",
        "- **Named Entity Recognition (NER)** locates spans of text that correspond to real‑world “entities” (people, organizations, locations, dates, etc.) and labels each span with its semantic category.  \n",
        "- **Part‑of‑Speech (POS) Tagging** assigns every single token a grammatical role (noun, verb, adjective, etc.), focusing on syntax rather than semantic identity.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Real‑World NER Applications**  \n",
        "- **Financial News Analytics:** Automatically extract company names, stock tickers, currencies, and event dates from news feeds to drive algorithmic trading, risk monitoring, or sentiment scoring.  \n",
        "- **Search Engines & QA Systems:** Disambiguate query terms (“Apple” the company vs. the fruit), link queries to knowledge‑graph entities, and return richer, entity‑centric answers and suggestions."
      ],
      "metadata": {
        "id": "mEUqdApfjK3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Q3: Scaled Dot‑Product Attention ===\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Q, K: (seq_len, d); V: (seq_len, d)\n",
        "    Returns attention_weights (seq_len, seq_len) and output (seq_len, d)\n",
        "    \"\"\"\n",
        "    d = Q.shape[-1]\n",
        "    # 1) matmul Q @ K^T\n",
        "    scores = Q @ K.T\n",
        "    # 2) scale\n",
        "    scaled_scores = scores / np.sqrt(d)\n",
        "    # 3) softmax\n",
        "    exp = np.exp(scaled_scores - np.max(scaled_scores, axis=-1, keepdims=True))\n",
        "    weights = exp / exp.sum(axis=-1, keepdims=True)\n",
        "    # 4) output = weights @ V\n",
        "    output = weights @ V\n",
        "    return weights, output\n",
        "\n",
        "# Test inputs\n",
        "Q = np.array([[1,0,1,0],\n",
        "              [0,1,0,1]], dtype=float)\n",
        "K = Q.copy()\n",
        "V = np.array([[1,2,3,4],\n",
        "              [5,6,7,8]], dtype=float)\n",
        "\n",
        "attn_w, out = scaled_dot_product_attention(Q, K, V)\n",
        "print(\"Attention weights (after softmax):\\n\", attn_w, \"\\n\")\n",
        "print(\"Final output:\\n\", out, \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkDs-WRJibtQ",
        "outputId": "c9db75f2-f89e-47d0-9240-0aec5b3dfba2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights (after softmax):\n",
            " [[0.73105858 0.26894142]\n",
            " [0.26894142 0.73105858]] \n",
            "\n",
            "Final output:\n",
            " [[2.07576569 3.07576569 4.07576569 5.07576569]\n",
            " [3.92423431 4.92423431 5.92423431 6.92423431]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Why divide by √d?**  \n",
        "When Q·Kᵀ grows large (for high-dimensional d), its softmax becomes extremely peaked, making gradients very small and training unstable. Dividing by √d scales the dot‑products down to a more moderate range, keeping the softmax output well‑conditioned and gradients healthy.\n",
        "\n",
        "---\n",
        "\n",
        "**2. How self‑attention captures word relationships**  \n",
        "Self‑attention lets each word’s representation attend to (i.e. compute weighted sums of) all other words in the sequence. By learning attention weights, the model can directly link related words—handling long‑range dependencies, resolving pronouns, or emphasizing context—without relying solely on sequential recurrences."
      ],
      "metadata": {
        "id": "epS3dh6ojNVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === Q4: Sentiment Analysis with HuggingFace ===\n",
        "sentiment = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def sentiment_example(text):\n",
        "    print(\"Input:\", text)\n",
        "    result = sentiment(text)[0]\n",
        "    print(f\"Sentiment: {result['label']}\")\n",
        "    print(f\"Confidence Score: {result['score']:.4f}\")\n",
        "\n",
        "# Run Q4 on the given sentence\n",
        "text_q4 = (\"Despite the high price, the performance of the new MacBook is outstanding.\")\n",
        "sentiment_example(text_q4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxu5OmHeifZz",
        "outputId": "591291dc-1d89-4c7a-d5e8-fe0efc90de18"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Despite the high price, the performance of the new MacBook is outstanding.\n",
            "Sentiment: POSITIVE\n",
            "Confidence Score: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. BERT vs. GPT**  \n",
        "- **Architecture:**  \n",
        "  - **BERT** is a **bidirectional encoder‑only** Transformer: every token attends to both left and right context.  \n",
        "  - **GPT** is a **unidirectional decoder‑only** Transformer: each token can only attend to previous tokens (left context).  \n",
        "- **Which uses which:**  \n",
        "  - BERT = **Encoder** stack  \n",
        "  - GPT  = **Decoder** stack  \n",
        "\n",
        "---\n",
        "\n",
        "**2. Benefits of Pre‑trained Models**  \n",
        "- **Rich Linguistic Knowledge:** Learned syntax, semantics, and world knowledge from massive text corpora.  \n",
        "- **Data Efficiency:** Downstream tasks need far fewer labeled examples (“fine‑tuning” vs. training from scratch).  \n",
        "- **Compute Savings:** Avoid the enormous cost of training tens or hundreds of millions of parameters from zero.  \n",
        "- **Strong Baselines:** Pre‑trained models achieve state‑of‑the‑art performance on diverse NLP tasks with minimal additional effort."
      ],
      "metadata": {
        "id": "628Lt1rojVJT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z33X9pA-jdu6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}